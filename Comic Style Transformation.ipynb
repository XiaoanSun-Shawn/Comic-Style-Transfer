{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Comic Style Transformation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"r-vhbKRbimO7","colab_type":"text"},"source":["**Source Code of \"Automatic Comic Style Transformation of Image\"**\n"]},{"cell_type":"markdown","metadata":{"id":"n0nIQxctMS5q","colab_type":"text"},"source":["The program majorly runs and is tested through Google colab since it offers free GPU with High performance.\n","\n","To use GPU, select \"Runtime/Change runtime type\" and choose GPU\n","\n","All the \"Test case\" are removable"]},{"cell_type":"code","metadata":{"id":"5gy2mRWvv0dG","colab_type":"code","colab":{}},"source":["\"\"\" The code is adapted from\n","1.\n","Title: <Neural style transfer>\n","Author: <Mark Daoust, Billy Lamberta, Minho Heo, Yash Katariya >\n","Date: <2018>\n","Code version: <python3.6>\n","Type: <Python program>\n","Availability: <https://www.tensorflow.org/tutorials/generative/style_transfer>\n","\n","2.\n","Title: <Neural Style Transfer: Creating Art with Deep Learning using tf.keras and eager execution>\n","Author: <Raymond Yuan>\n","Date: <2018-08-03>\n","Code version: <python3.6>\n","Type: <Blog>\n","Availability: <https://blog.tensorflow.org/2018/08/neural-style-transfer-creating-art-with-deep-learning.html>\n","\n","\"\"\"\n","\n","\"\"\" The code uses the following libraries:\n","1.\n","Title: <TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems>\n","Author: <Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo,\n","Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis,\n","Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow,\n","Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia,\n","Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster,\n","Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens,\n","Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker,\n","Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas,\n","Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,\n","Yuan Yu, and Xiaoqiang Zheng>\n","Date: <2015>\n","Code version: <python3.6>\n","Type: <Software available from tensorflow.org>\n","Availability: <http://tensorflow.org/>\n","\n","2.\n","Title: <Open Source Computer Vision Library>\n","Author: <OpenCV>\n","Date: <2015>\n","Type: <Software available from opencv.org>\n","Availability: <http://docs.opencv.org/>\n","\n","3.\n","Title: <IPython: a system for interactive scientific computing>\n","Author: <Perez, Fernando and Granger, Brian E>\n","Date: <2007>\n","Journal: <Computing in Science & Engineering>\n","Volume: <9> \n","Number: <3> \n","Date: <2007>,\n","Publisher: <IEEE> \n","Availability: <https://github.com/ipython/ipython>\n","\n","\"\"\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4wgEdf510t81","colab_type":"code","colab":{}},"source":["# Load the google drive before running the program \n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nFi2r_zV1JQF","colab_type":"code","colab":{}},"source":["# Please load the google drive before running the program\n","import os \n","\n","# Swtich the root directory to the image directory (Please adjust the path if required)\n","image_dir = \"/content/drive/My Drive/source code/images\"\n","os.chdir(image_dir)\n","\n","# Check whether the directory loaded correctly. It should include style images and comic images\n","os.listdir(image_dir)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"a33OiH6P2Gcc","colab_type":"code","colab":{}},"source":["# Import necessary library\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import IPython.display\n","import numpy as np\n","import time\n","import cv2\n","from tensorflow.python.keras import *\n","from tensorflow.python.keras.preprocessing import image as kp_image\n","from PIL import Image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tSl_uyLM2Tc8","colab_type":"code","colab":{}},"source":["# Global values used in the model\n","# Path to styel and content images (Please adjust the path if required)\n","CONTENT_IMAGE='content_1.jpg' # You can change the content image through change this to filename of your own image\n","STYLE_IMAGE='comic_1.jpg' # You can change the style image through change this to filename of your own image\n","STYLE_IMAGE_2='comic_2.jpg'\n","\n","# The size of image\n","HEIGHT=800\n","WEIGHT=600\n","CHANNEL=3\n","\n","# The pre-trained model that we want to use for NST\n","MODEL=tf.keras.applications.vgg19# VGG is default in the paper\n","MODEL_VGG19=MODEL.VGG19(include_top=False,weights=\"imagenet\")\n","\n","# Content and Style layers that we want to extract the content and styler features from\n","CONTENT_LAYERS=['block3_conv3']\n","STYLE_LAYERS=['block1_conv1','block2_conv1','block3_conv1','block4_conv1','block5_conv1']\n","\n","# Weights for each layer and number of layers\n","STYLE_LAEYR_WEIGHTS=[0.1,0.1,0.35,0.35,0.1]\n","CONTENT_LAYER_WEIGHTS=[1]\n","NUM_CONTENT_LAYERS = len(CONTENT_LAYERS)\n","NUM_STYLE_LAYERS = len(STYLE_LAYERS)\n","\n","# The average BGR of ImageNet since VGG-19 will substact each pixel by the average value for preprocessing\n","# These values are also used in deprocessing the image\n","NORM_MEANS=np.array([103.939, 116.779, 123.68])\n","MIN_VALS=-NORM_MEANS\n","MAX_VALS=255-NORM_MEANS\n","\n","# Weights for each sections when computing the total loss\n","TOTAL_VAR_WEIGHT=1e3\n","STYLE_WEIGHT=1e1\n","CONTENT_WEIGHT=1e4\n","STYLE_WEIGHT_2=1e1\n","\n","# Adust the proportion of noise when generate the original image\n","NOISE_RATIO=0\n","\n","# Create kernel used for dilation/erosion\n","KERNEL=cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3,3))\n","\n","# Used to store the best image for continuing the training\n","BEST_IMAGE_STORE=None"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wTvXJfTamJMd","colab_type":"code","colab":{}},"source":["# Test case - output the model\n","vgg_test=MODEL_VGG19\n","vgg_test.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xuOtlOgw26k3","colab_type":"code","colab":{}},"source":["def load_image(image):\n","  \"\"\"load the image from the directory\n","\n","  Args:\n","    image: the path of the image \n","\n","  Returns:\n","    image: image array with shape (1,Weight,Height,Channel)\n","  \n","  \"\"\"\n","  image=Image.open(image)\n","  image=image.resize((WEIGHT, HEIGHT))\n","  image=kp_image.img_to_array(image)\n","  image=np.reshape(image,((1,)+np.shape(image)))\n","  return image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ol9DkIjH3Cdw","colab_type":"code","colab":{}},"source":["def load_and_process_image(image,enhance_edges=False):\n","  \"\"\"load and pre-process the image from the directory\n","\n","  For CNN model, it is beneficial to preprocess the input data for normalization.\n","  This function is used to realize loading and preprocessing the input data. If enhancem_edges\n","  is True, it will enhance the edges of the input image before preprocessing.\n","  \n","  Args:\n","    image: The path of the image\n","    enhance_edges: Control whether the edges of the input image should be enhanced\n","  \n","  Returns:\n","    image: A image arrary preprocessed based on the model\n","\n","  \"\"\"\n","  image=load_image(image)\n","\n","  # Whether enhance the edges in the image\n","  if enhance_edges==True:\n","    image=image[0]\n","    enhanced_image,_,_=edge_enhance(image,image)\n","    plot_image_enhance=np.clip(enhanced_image,0,255).astype('uint8')\n","    image=np.reshape(plot_image_enhance,((1,)+np.shape(plot_image_enhance)))  \n","\n","  image=MODEL.preprocess_input(image)\n","  \n","  return image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ck3EpY183HTC","colab_type":"code","colab":{}},"source":["def deprocess_image(image):\n","  \"\"\"Deprocess the image to obtain an image that is suitable for display\n","\n","    Args:\n","      image: Pre-processed image\n","\n","    Returns:\n","      image: Deprocessed image\n","\n","  \"\"\" \n","  # Remove the extra dimension added when we load the image\n","  if len(image.shape)==4:\n","    image=np.squeeze(image,0)\n","  \n","  # Perform the inverse of the preprocessiing step, these values are average values for BGR channels\n","  image[:,:,0]+=103.939\n","  image[:,:,1]+=116.779\n","  image[:,:,2]+=123.68\n","  image=image[:,:,::-1] # BGR<->RGB\n","\n","  # Clip the generated imges' value to [0, 255] for display\n","  image=np.clip(image,0,255).astype('uint8')\n","  \n","  return image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"osx19q3k_8a_","colab_type":"code","colab":{}},"source":["# Test case - input image display\n","image_C=load_image(CONTENT_IMAGE)[0].astype('uint8')\n","image_S=load_image(STYLE_IMAGE)[0].astype('uint8')\n","image_S_2=load_image(STYLE_IMAGE_2)[0].astype('uint8')\n","plt.subplot(1,3,1)\n","plt.imshow(image_C)\n","plt.subplot(1,3,2)\n","plt.imshow(image_S)\n","plt.subplot(1,3,3)\n","plt.imshow(image_S_2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"leRsScZH1TCh","colab_type":"code","colab":{}},"source":["def edge_extractor(image):\n","  \"\"\"Extract edges from image\n","\n","    Args:\n","      image: The path of image\n","\n","    Returns:\n","      edges: The edges of the image\n","\n","  \"\"\"\n","  edges=cv2.GaussianBlur(image.astype(np.uint8),(3,3),4)\n","  edges=cv2.Canny(edges,200,300)\n","\n","  # Invert the extracted edges map for further enhancement \n","  edges=cv2.bitwise_not(edges)\n","\n","  return edges"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"oERdhSiR1U2R","colab_type":"code","colab":{}},"source":["def edge_enhance(image_get_edge,image_result):\n","  \"\"\"Enhance edges of image\n","\n","  This function will enhance the edges of an image through adding\n","  the extracted edges back to the image after dilation.\n","\n","    Args:\n","      image_get_edge: The path of image we want to extract the edges\n","      image_result: The image we want to enhance the edges.\n","    Returns:\n","      enhanced_image: The image after enhancement\n","      edges: The edge map after enhancement\n","      st_edges: The edge map before enhancement\n","  \"\"\"\n","  edges=edge_extractor(image_get_edge)\n","  st_edges=edges\n","\n","  # Since the edges map is inverted when extracted, erosion would be used for dilation purpose\n","  edges=cv2.morphologyEx(edges, cv2.MORPH_ERODE, KERNEL, iterations=1)\n","\n","  # Cover the edges back to the image\n","  enhanced_image=cv2.bitwise_and(image_result,image_result,mask=edges)\n","\n","  return enhanced_image,edges,st_edges"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"laaEkY6k3P7V","colab_type":"code","colab":{}},"source":["def get_model(model):\n","  \"\"\"Create the model used to get the output values from intermediate layers\n","\n","  The function will load the model and create a new model based on the layers\n","  that we choose to get the output values from\n","\n","    Args:\n","      model: Yhe pre-trained model we want to use\n","\n","    Returns:\n","      model: The new model created from the input model based on layers we choose\n","\n","  \"\"\"\n","  # Turn off the training since we want keep the weights the same.\n","  model.trainable=False\n","\n","  # Get the output feature layers corresponding to the style and content layers we choose.\n","  style_outputs=[model.get_layer(name).output for name in STYLE_LAYERS]\n","  content_outputs=[model.get_layer(name).output for name in CONTENT_LAYERS]\n","\n","  # Create the new model and turn off the traning.\n","  model=models.Model(model.input,style_outputs+content_outputs)\n","  for layer in model.layers:\n","    layer.trainable=False\n","\n","  return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_83XH5kswL75","colab_type":"code","colab":{}},"source":["# Test case - the layers read from the model\n","style_outputs=[vgg_test.get_layer(name).output for name in STYLE_LAYERS]\n","content_outputs=[vgg_test.get_layer(name).output for name in CONTENT_LAYERS]\n","style_outputs+content_outputs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BG68TeOdqhOc","colab_type":"code","colab":{}},"source":["# Test case - the model we created\n","#model=get_model(MODEL_VGG19)\n","#model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xHK99_o6mNEm","colab_type":"code","colab":{}},"source":["def get_content_feature(model,content_image,enhance_edges=False):\n","  \"\"\"Compute the content features from the model.\n","\n","  This function will load and preprocess the content image. \n","  Then it will input it into the model to get the intermediate \n","  outputs from the layer we choose.\n","  \n","  Arguments:\n","    model: The model that we use.\n","    content_image: The path to the content image.\n","    enhance_edges: Control whether the edges of the input image should be enhanced\n","    \n","  Returns:\n","    content_features: The content features. \n","  \"\"\"\n","  content_image=load_and_process_image(content_image,enhance_edges)\n","  content_outputs=model(content_image)\n","\n","  # The reason of code style_layer[0] is to reduce the dimension, converting [1,n_h,n_w,n_c] into [n_h,n_w,n_c].\n","  # NUM_STYLE_LAYERS used to select the whether we want the style or content output since they are superimposed together when creating the model\n","  content_features=[content_layer[0] for content_layer in content_outputs[NUM_STYLE_LAYERS:]]\n","  \n","  return content_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4XwLIhqmo5I","colab_type":"code","colab":{}},"source":["def get_style_feature(model,style_image):\n","  \"\"\"Compute the style features from the model.\n","\n","  This function will load and preprocess the style image. \n","  Then it will input it into the model to get the intermediate \n","  outputs from the layer we choose.\n","  \n","  Arguments:\n","    model: The model that we use.\n","    style_image: The path to the style image.\n","    \n","  Returns:\n","    content_features: The style features. \n","  \"\"\"\n","  style_image=load_and_process_image(style_image)\n","  style_outputs=model(style_image)\n","  style_features=[style_layer[0] for style_layer in style_outputs[:NUM_STYLE_LAYERS]]\n","\n","  return style_features"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KB05Ng4y3Vwl","colab_type":"code","colab":{}},"source":["def get_content_loss(base_content, target):\n","  \"\"\"Calculate the content loss. \n","\n","    Args:\n","      base_content: The content output of generated image \n","      target: The content output of content image\n","    \n","    Returns:\n","      loss: The content loss\n","  \"\"\"\n","  loss=tf.reduce_mean(tf.square(base_content - target))\n","\n","  return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CPl7laDVwJM1","colab_type":"code","colab":{}},"source":["def gram_matrix(image):\n","  \"\"\"Calculate the Gram matrix  \n","\n","    Args:\n","     image: the input tensor of the image.\n","    \n","    Returns:\n","      gram: The Gram matrix\n","\n","  \"\"\" \n","  # Flatten the image and set channel to the first.\n","  A=tf.reshape(image,shape=[-1,int(image.shape[-1])])\n","\n","  n=tf.shape(A)[0]\n","  gram=tf.matmul(tf.transpose(A), A)\n","  gram=gram/tf.cast(n, tf.float32)\n","  \n","  return gram"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q5InMcAS3Zqi","colab_type":"code","colab":{}},"source":["def get_style_loss(base_style, gram_target):\n","  \"\"\"Calculate the style loss. \n","\n","    Args:\n","      base_style: The style output of generated image \n","      target: The style output of content image\n","    \n","    Returns:\n","      loss: The style loss\n","  \"\"\"\n","  gram_style=gram_matrix(base_style)\n","  loss=tf.reduce_mean(tf.square(gram_style-gram_target))\n","  \n","  return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"So9sdfvw3dwo","colab_type":"code","colab":{}},"source":["def get_loss_sum(model, style_weight,content_weight, generated_image, gram_style_features, \n","                 content_features,style_layer_weights,content_layer_weights,gram_style_features_2=None,style_weight_2=0):\n","  \"\"\"Calculate the loss sum\n","\n","    Args:\n","      model: The CNN model used for extract the features\n","      style_weight: The proportion of style cost, also could serve as normalization\n","      content_weight: The proportion of content cost, also could serve as normalization\n","      generated_image: The generated image\n","      gram_style_features: The Gram matrix of style image\n","      content_features: The content features from content image\n","      style_layer_weights: Weights for each style layer\n","      content_layer_weights: Weights for each content layer\n","      gram_style_feature_2: The Gram matrix of style image 2, default to be None if there's no second style image\n","      style_weight_2: The proportion of style cost 2, also could serve as normalization,\n","              default to be 0 if there's no second style image\n","    \n","    Returns:\n","      loss: The sum of content cost, style cost and style cost 2 (if exists)\n","      style_cost: The sum of style costs (if style cost 2 exists)\n","      content_cost: The content cost\n","  \"\"\"\n","  # Input the generated image to the model\n","  model_outputs=model(generated_image)\n","  \n","  # Separate the style features and content features generated through the model\n","  style_generated_outputs=model_outputs[:NUM_STYLE_LAYERS]\n","  content_generated_outputs=model_outputs[NUM_STYLE_LAYERS:]\n","  \n","  # The initial cost\n","  style_cost=0\n","  style_cost_2=0\n","  content_cost=0\n","\n","  # Counters used for giving each ConvNet layer with corresponding weight\n","  style_layer_count=0\n","  style_layer_count_2=0\n","  content_layer_count=0\n","  \n","  # Calculate the sum of content loss from each layer\n","  dict_content=zip(content_features,content_generated_outputs)\n","  for target_content,generated_content in dict_content:\n","    content_cost+=content_layer_weights[content_layer_count]* get_content_loss(generated_content[0],target_content)\n","    content_layer_count+=1\n","    \n","  # Calculate the sum of style loss from each layer\n","  dict_style=zip(gram_style_features,style_generated_outputs) \n","  for target_style,generated_style in dict_style:\n","    style_cost+=style_layer_weights[style_layer_count]*get_style_loss(generated_style[0],target_style)\n","    style_layer_count+=1\n","\n","  # Calculate the sum of style loss 2 from each layer if there's second style image exists\n","  if gram_style_features_2 != None:\n","    dict_style_2=zip(gram_style_features_2,style_generated_outputs)\n","    for target_style_2,generated_style in dict_style_2:\n","      style_cost_2+=style_layer_weights[style_layer_count_2]*get_style_loss(generated_style[0],target_style_2)\n","      style_layer_count_2+=1\n","  \n","  # Calculate the total style loss\n","  style_cost=style_weight*style_cost+style_weight_2*style_cost_2\n","  content_cost=content_weight*content_cost\n","\n","  # Calculate the total loss\n","  loss=style_cost+content_cost\n","\n","  return loss,style_cost,content_cost"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAUD-3g_IPZ5","colab_type":"code","colab":{}},"source":["def generate_image(content_image,noise_ratio=NOISE_RATIO,enhance_edges=False):\n","  \"\"\"Generate the initial image with or without white noise\n","\n","  This function is used to generate the initial image. With high noise_ratio,\n","  The initial image would includes more white noise. If the noise_ratio is 0,\n","  The initial image would be the same as the content image.\n","\n","    Args:\n","      content_image: The content image\n","      noise_ratio: The proportion of white noise in generated image\n","      enhance_edges: Control whether the edges of the input image should be enhanced\n","    \n","    Returns:\n","      generated_image: The generated image\n","  \"\"\"\n","  image=load_and_process_image(content_image,enhance_edges)\n","\n","  # Create a wthie noise image/content-image-realted image\n","  noise_image=np.random.uniform(-20,20,(1,HEIGHT,WEIGHT,CHANNEL))\n","  generated_image=noise_ratio*noise_image+(1-noise_ratio)*image\n","\n","  return generated_image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0VwCZgeB3Hhr","colab_type":"code","colab":{}},"source":["def total_variation_loss(image,tv_weight):\n","  \"\"\"Reduce the noise exists in the generated image\n","\n","  This function will calculate the total variation loss in order to reduce\n","  the difference between neighboring pixels in order to reduce the noise and\n","  make the image look smooth.\n","\n","    Args:\n","      image: The generated image\n","    \n","    Returns:\n","      loss: The total variation loss\n","  \"\"\"\n","  x_var=image[:,:,1:,:]-image[:,:,:-1,:]\n","  y_var=image[:,1:,:,:]-image[:,:-1,:,:]\n","  loss=tv_weight*(tf.reduce_mean(x_var**2)+tf.reduce_mean(y_var**2))\n","  return loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ss-qJVgvdiC","colab_type":"code","colab":{}},"source":["def one_training_step(cfg,opt,tv_weight):\n","  \"\"\"Calculate and update the gradient\n","\n","    Args:\n","      cfg: config including all required parameters for calculate the loss sum\n","      opt_learning_rate: learning rate\n","    \n","    Returns:\n","      loss: Total loss\n","      style_cost: Style loss\n","      content_cost: Content loss\n","      image: The generated after one update\n","      var_loss: Total variation loss\n","  \"\"\"\n","  image=cfg['generated_image']\n","\n","  # Apply gradient descent\n","  with tf.GradientTape() as tape: \n","    all_loss=get_loss_sum(**cfg)\n","    var_loss=total_variation_loss(image,tv_weight)\n","    total_loss=all_loss[0]+var_loss\n","  grads=tape.gradient(total_loss,image)\n","  opt.apply_gradients([(grads,image)])\n","\n","  clipped=tf.clip_by_value(image,MIN_VALS,MAX_VALS)\n","  image=image.assign(clipped)\n","  loss,style_cost,content_cost=all_loss\n","\n","  return loss,style_cost,content_cost,image,var_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hgc-OHJCHTTf","colab_type":"code","colab":{}},"source":["def convert_to_original_colors(content_image, generated_image):\n","  \"\"\"Convert the generated image to its original color\n","  \n","  This function will adjust the color of generated image in YCbCr space.\n","  It would keep the luma component the same as generated image but convert\n","  its blue-difference and red-difference chroma components into original image's\n","  corresponding components.\n","\n","    Args:\n","      content_image: The content image\n","      generated_image: The generated image\n","    \n","    Returns:\n","      converted_image: The generated image whose color has been converted to original color\n","  \"\"\"\n","  content_image=load_image(content_image)[0].astype('uint8')\n","  \n","  # Switch the color sapce to YCbCr\n","  content_cvt=cv2.cvtColor(content_image,cv2.COLOR_BGR2YCR_CB)\n","  generated_cvt=cv2.cvtColor(generated_image,cv2.COLOR_BGR2YCR_CB)\n","  c1=cv2.split(generated_cvt)[0]\n","  c2=cv2.split(content_cvt)[1]\n","  c3=cv2.split(content_cvt)[2]\n","  merged=cv2.merge((c1,c2,c3))\n","  converted_image=cv2.cvtColor(merged,cv2.COLOR_YCR_CB2BGR).astype('uint8')\n","\n","  return converted_image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XoOtk0Rx3nif","colab_type":"code","colab":{}},"source":["def run_NST(model,content_image,style_image,style_layer_weights,content_layer_weights,\n","            num_iterations=1000,opt_learning_rate=1,content_weight=1,style_weight=1,\n","            continue_training=False,style_image_2=None,style_weight_2=0,tv_weight=0,enhance_edges=False): \n","  \"\"\"Implement the Neural Style Transfer\n","\n","    Args:\n","      model: The model used for extracting the features\n","      content_image: The target content image\n","      style_image: The target style image\n","      style_layer_weights: The weights for each layers when calculating the style cost sum\n","      content_layer_weights: The weights for each layers when calculating the content cost sum\n","      num_iterations: Number of iteration\n","      opt_learning_rate: Learning rate\n","      content_weight: Weight of content cost\n","      style_weight: Weight of style cost\n","      continue_training: Whether to contiune the training based on the best gerenated image obtained\n","      style_image_2: The second target style image(if exists)\n","      style_weight_2: Weight of second style image\n","      tv_weight: Weight of total variation loss\n","      enhance_edges: Whether to enhance the edges of content image.\n","    \n","    Returns:\n","      best_image: The best generated image \n","      best_loss: The lowest loss\n","      array_loss: The loss array during the training \n","      array_style_loss: The style loss array during the training\n","      array_content_loss: The content loss array during the training\n","      array_TV_loss: The Total variation loss aray during the training\n","      images: Image set reflecting the process of training\n","      image_convert_color: The best generated image with color of target content image\n","  \"\"\"\n","  global BEST_IMAGE_STORE\n","\n","  # The second style image is empty in default  \n","  gram_style_features_2=None\n","\n","  # Get the model used for feature extracting\n","  model=get_model(model) \n","\n","  # Get the style and content features from the chosen layer of model\n","  style_features=get_style_feature(model,style_image)\n","  content_features=get_content_feature(model,content_image,enhance_edges)\n","\n","  # Calculate the Gram matrix for style feature\n","  gram_style_features=[gram_matrix(style_feature) for style_feature in style_features]\n","\n","  # If second style image exists, get its feature and calculate its Gram matrix\n","  if style_image_2!=None:\n","    style_features_2=get_style_feature(model,STYLE_IMAGE_2)\n","    gram_style_features_2=[gram_matrix(style_feature_2) for style_feature_2 in style_features_2]\n","  \n","  # Set initial image for this training, if continue traning is True, it will use the current best generated result as the initial image.\n","  if continue_training==False:\n","    generated_image=generate_image(content_image,NOISE_RATIO,enhance_edges)\n","  elif BEST_IMAGE_STORE is None:\n","    generated_image=generate_image(content_image,NOISE_RATIO,enhance_edges)\n","  else:\n","    generated_image=BEST_IMAGE_STORE\n","\n","  # Set generated_image as Variable for applying graident descent.\n","  generated_image=tf.Variable(generated_image,dtype=tf.float32)\n","\n","  # Iteration counter.\n","  iter_count=1\n","  \n","  # Initialize the best loss and best image for storage in the future\n","  best_loss=float('inf')\n","  best_image=None\n","  \n","  # Create config including all required parameters for calculate the loss sum\n","  cfg={\n","      'model': model,\n","      'style_weight': style_weight,\n","      'content_weight':content_weight,\n","      'generated_image': generated_image,\n","      'gram_style_features': gram_style_features,\n","      'gram_style_features_2': gram_style_features_2,\n","      'content_features': content_features,\n","      'style_layer_weights':style_layer_weights,\n","      'content_layer_weights':content_layer_weights,\n","      'style_weight_2':style_weight_2\n","  }\n","\n","  # Create optimizer\n","  opt=tf.optimizers.Adam(learning_rate=opt_learning_rate,beta_1=0.99,epsilon=1e-1)\n","\n","  # Set the layout for displaying the generated image set at the end\n","  num_rows=2\n","  num_cols=5\n","\n","  # Set the interval of recording the loss\n","  loss_store_interval=5\n","\n","  # Set the interval of displaying generated image during the training\n","  display_interval=num_iterations/(num_rows*num_cols)\n","\n","  # Initialize the Timmer\n","  start_time=time.time()\n","  end_time=time.time()\n","  global_start=time.time()\n","  global_end=time.time()   \n","  \n","  # Initialize the loss arrays and image set\n","  images=[]\n","  array_loss=[]\n","  array_style_loss=[]\n","  array_content_loss=[]\n","  array_TV_loss=[]\n","\n","  # Start the training\n","  for i in range(num_iterations):\n","    # One step of training\n","    loss,style_cost,content_cost,generated_image,var_loss=one_training_step(cfg,opt,tv_weight)\n","    end_time=time.time() \n","    \n","    # Store the best loss and best image\n","    if loss<best_loss:\n","      # Update best loss and best image from total loss. \n","      best_loss=loss\n","      best_image=deprocess_image(generated_image.numpy())\n","\n","    # Record the loss, skip the first 25 rounds since the loss is very huge at the begining, which will affect observation\n","    if i%loss_store_interval==0 and i>=25: \n","      array_loss.append(loss)\n","      array_style_loss.append(style_cost)\n","      array_content_loss.append(content_cost)\n","      array_TV_loss.append(var_loss)\n","\n","    # Display current training situation\n","    if i%display_interval==0:# Start timer\n","      average_time_interval=(end_time-start_time)/display_interval\n","      start_time=time.time()\n","      plot_image=deprocess_image(generated_image.numpy())\n","\n","      # Store the image\n","      images.append(plot_image)\n","\n","      # Display the generated image in training\n","      IPython.display.clear_output(wait=True)\n","      IPython.display.display_png(Image.fromarray(plot_image))\n","\n","      # Display related data\n","      print('Iteration:{}'.format(i))        \n","      print('Total loss:{:.4e}'.format(loss))\n","      print('Style loss:{:.4e}'.format(style_cost))\n","      print('Content loss:{:.4e}'.format(content_cost))\n","      print('TV loss:{:.4e}'.format(var_loss))\n","      print('Average time for previous {:.1f} iterations:{:.4f}s'.format(display_interval,average_time_interval))\n","\n","  # Record the final loss\n","  array_loss.append(loss)\n","  array_style_loss.append(style_cost)\n","  array_content_loss.append(content_cost)\n","\n","  # Caculate total elapsed time\n","  global_end=time.time()\n","\n","  # Display all images stored\n","  IPython.display.clear_output(wait=True)\n","  plt.figure(figsize=(14,4))\n","  for i,image in enumerate(images):\n","      plt.subplot(num_rows,num_cols,i+1)\n","      plt.imshow(image)\n","      plt.xticks([])\n","      plt.yticks([])\n","\n","  # Display the related data\n","  print('Total time:{:.4f}s'.format(global_end - global_start))\n","  print('Total loss:{:.4e}'.format(loss))\n","  print('Style loss:{:.4e}'.format(style_cost))\n","  print('Content loss:{:.4e}'.format(content_cost))\n","  print('TV loss:{:.4e}'.format(var_loss))\n","  print('Average time per iteation:{:.4f}s'.format((global_end - global_start)/num_iterations))\n","  \n","  # Convert the generated image to the color of target content image\n","  image_convert_color=convert_to_original_colors(content_image,best_image)\n","\n","  # The following code is used for continuing the training during experiment, which is not necessary\n","  BEST_IMAGE_STORE=best_image\n","  BEST_IMAGE_STORE=np.reshape(best_image,((1,)+np.shape(best_image)))\n","  BEST_IMAGE_STORE=MODEL.preprocess_input(BEST_IMAGE_STORE)\n","\n","  return best_image,best_loss,array_loss,array_style_loss,array_content_loss,array_TV_loss,images,image_convert_color"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AyPyLsm94CIy","colab_type":"code","colab":{}},"source":["# Perform training， feel free to adjust the parameters\n","best_image,best_loss,array_loss,array_style_loss,array_content_loss,array_TV_loss,result_images,image_cvt=run_NST(\n","    model=MODEL_VGG19,\n","    content_image=CONTENT_IMAGE, \n","    style_image=STYLE_IMAGE, \n","    style_layer_weights=STYLE_LAEYR_WEIGHTS,\n","    content_layer_weights=CONTENT_LAYER_WEIGHTS,\n","    tv_weight=TOTAL_VAR_WEIGHT,\n","    num_iterations=1000,\n","    opt_learning_rate=5,\n","    content_weight=CONTENT_WEIGHT,\n","    style_weight=STYLE_WEIGHT,\n","    continue_training=False,\n","    enhance_edges=True,\n","    #style_image_2=STYLE_IMAGE_2, # Remove the Comment symbol to realize multiple style combination\n","    #style_weight_2=STYLE_WEIGHT_2 # Remove the Comment symbol to realize multiple style combination\n","    )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1uO4quEC5lsC","colab_type":"code","colab":{}},"source":["# Display the best image generated\n","IPython.display.display_png(Image.fromarray(best_image))\n","\n","# Display the best image generated with color conversion\n","IPython.display.display_png(Image.fromarray(image_cvt))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u0seQ-8ZOFhG","colab_type":"code","colab":{}},"source":["def save_image_sets(images):\n","  \"\"\"Save the image set\n","\n","    Args:\n","      images: Image set including the generated images during the training\n","    \n","    Returns:\n","  \"\"\"\n","  for i,image in enumerate(images):\n","    Image.fromarray(image).save('generated_image_'+str(i+1) +'.jpg')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5IVTBsF6mxF","colab_type":"code","colab":{}},"source":["def save_best_image(best_image):\n","  \"\"\"Save the best image\n","\n","    Args:\n","      best_image: Best image generated\n","    \n","    Returns:\n","  \"\"\"\n","  Image.fromarray(best_image).save('generated_image.jpg')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ggU2Z5f2UfFn","colab_type":"code","colab":{}},"source":["def save_loss_plot(array_loss,array_style_loss,array_content_loss):\n","  \"\"\"Plot and save the loss figures\n","\n","    Args:\n","      array_loss: Array of total loss\n","      array_style_loss: Array of style loss\n","      array_content_loss: Array of content loss\n","    \n","    Returns:\n","  \"\"\"\n","  plt.figure(figsize=(10,16))\n","  plt.subplot(3,1,1)\n","  plt.plot(array_loss)\n","  plt.title('total loss')\n","  plt.subplot(3,1,2)\n","  plt.plot(array_style_loss)\n","  plt.title('style loss')\n","  plt.subplot(3,1,3)\n","  plt.plot(array_content_loss)\n","  plt.title('content loss')\n","  plt.savefig('loss figure')\n","  plt.close\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fWKFWOOT62zR","colab_type":"code","colab":{}},"source":["# Save results to the file\n","save_image_sets(result_images)\n","save_best_image(best_image)\n","save_loss_plot(array_loss,array_style_loss,array_content_loss)\n","save_best_image(image_cvt)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgYDxAmr3Zk6","colab_type":"code","colab":{}},"source":["# Plot the total variation loss figures\n","plt.plot(array_TV_loss)\n","plt.title('Total variation loss')"],"execution_count":0,"outputs":[]}]}